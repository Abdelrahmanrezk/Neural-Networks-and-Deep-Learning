{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases_v4a import *\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (9.0, 7.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_shallow_NN(n_x,n_h,n_y):\n",
    "    '''\n",
    "        This function I used to initialize a shallow Nerual Network with just:\n",
    "        input layer & 1  hidden layer & output layer as n_x,n_h,n_y \n",
    "        and n_x,n_h,n_y  for number of nodes or features in each layer\n",
    "        also we use it to initialize our parameters using these params\n",
    "        \n",
    "        function return\n",
    "            This function will return a python dictionary that has a key of param and its value\n",
    "    '''\n",
    "    '''\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    '''\n",
    "    W1 = np.random.randn(n_h,n_x) * 0.01 # multiple each value of .01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h) * 0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    parameters = {\n",
    "        'W1' : W1,\n",
    "        'b1' : b1,\n",
    "        'W2' : W2,\n",
    "        'b2' : b2\n",
    "    }\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = (2, 3)\n",
      "b1 = (2, 1)\n",
      "W2 = (1, 2)\n",
      "b2 = (1, 1)\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_shallow_NN(3,2,1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"].shape))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"].shape))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"].shape))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_deep_NN(layer_dims):\n",
    "    '''\n",
    "        This another function I used to initialize a deep Nerual Network with:\n",
    "        layer_dims, its a python list which contain size of each layer \n",
    "        and include the input layer so we need to start our loop from layer 1 which first hidden layer\n",
    "        that help you create a multiple NN layers depends on your size of each layer given to layer_dims\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "    '''\n",
    "    \n",
    "    L = len(layer_dims) # first we get length of the layers of out NN \n",
    "    parameters = {}\n",
    "    for l in range(1,L):\n",
    "        '''\n",
    "             start from layer 1 because actually we do not get weight for input layer \n",
    "             we use it to get weights of first hidden layer \n",
    "             and then use size of first hidden layer to get scond hidden layer weights and so on to output layer\n",
    "             general role for that \n",
    "             \"Weight of any Layer L = (size of its nodes, size of previous layer nodes)\"\n",
    "             \"bias of any Layer L = (size of its nodes, 1)\"\n",
    "        '''\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1]) * .01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        \n",
    "    \n",
    "    return parameters\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.00319039 -0.0024937   0.01462108 -0.02060141 -0.00322417]\n",
      " [-0.00384054  0.01133769 -0.01099891 -0.00172428 -0.00877858]\n",
      " [ 0.00042214  0.00582815 -0.01100619  0.01144724  0.00901591]\n",
      " [ 0.00502494  0.00900856 -0.00683728 -0.0012289  -0.00935769]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.00267888  0.00530355 -0.00691661 -0.00396754]\n",
      " [-0.00687173 -0.00845206 -0.00671246 -0.00012665]\n",
      " [-0.0111731   0.00234416  0.01659802  0.00742044]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# you can give it as you need multiple layers but then you should loop to print all w and b\n",
    "parameters = initialize_deep_NN([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A,W,b):\n",
    "    '''\n",
    "        This function used to calculate Z which is need:\n",
    "        A_previous we send as A, and W of the layer and b of the layer\n",
    "        then all of these values saved and returned\n",
    "    '''\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z     =     np.dot(W,A) +b\n",
    "    cache = (A,W,b)\n",
    "#     print(Z.shape)\n",
    "#     print(A.shape)\n",
    "#     print(W.shape)\n",
    "#     print(b.shape)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[-0.98253931  3.92578674]]\n",
      "linear_cache = (array([[-0.19183555, -0.88762896],\n",
      "       [-0.74715829,  1.6924546 ],\n",
      "       [ 0.05080775, -0.63699565]]), array([[0.19091548, 2.10025514, 0.12015895]]), array([[0.61720311]]))\n"
     ]
    }
   ],
   "source": [
    "A = np.random.randn(3,2)\n",
    "W = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))\n",
    "print(\"linear_cache = \" + str(linear_cache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_previous,W,b, activation_fun_name):\n",
    "    '''\n",
    "        This function used to return the activation value A for specific layer and also save these values\n",
    "        all of saved values in cache we use it for backward\n",
    "        also the function depend on what is activation function you need to use for this layer\n",
    "        because we have multiple activation function we can use like Relu, tanch and sigmoid\n",
    "        but actually most use of relu and sigmoid for only output layer\n",
    "    '''\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    if activation_fun_name == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_previous, W, b) # function we create above\n",
    "        A, activation_cache = sigmoid(Z) # return activation cache which save value of Z and Activation of the layer\n",
    "        print(A.shape)\n",
    "    elif activation_fun_name == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_previous, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        print(A.shape)\n",
    "        print(activation_cache.shape)\n",
    "    \n",
    "    '''\n",
    "        now the variable cache contain the the Z in activation_cache value\n",
    "        and contain A,W,b for this layer, A here is A_prev that used with the w,b paramter to get\n",
    "        activation of this layer\n",
    "    '''\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache # activation of the layer we in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "With sigmoid: A = [[0.50338007 0.60631959]]\n",
      "(1, 2)\n",
      "(1, 2)\n",
      "With ReLU: A = [[0.01352047 0.4318678 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "A_prev = np.random.randn(3,2)\n",
    "W = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_forward_model_shallow_or_deep_NN(X, parameters):\n",
    "    '''\n",
    "        here is our forward process we will make just we need X which the input layer\n",
    "        and parameters for each hidden layer and output layer and we get length of NN layer via len(parameters)//2\n",
    "        because parameters has all w,b of all layers and for that layers number is len(parameters)//2\n",
    "        at same time we need to save all layers paramters we use so now we will use:\n",
    "        caches to store each cache for each layer and cache it self contain 2 values\n",
    "        1- linear_cache which has three values A_previous,W,b\n",
    "        2- activation_cache which has the Z of the  layer we used linear_cache variables to calculate it\n",
    "    '''\n",
    "    \n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "        \n",
    "    caches = []\n",
    "    L      = len(parameters) // 2\n",
    "    A = X # its means input layer we call it as A of 0\n",
    "    for l in range(1,L):\n",
    "        '''\n",
    "            actually this loop will go all only hidden layer because when l is == L will terminated\n",
    "            then we will calculate output layer after loop using sigmoid function\n",
    "        '''\n",
    "        A_prev = A\n",
    "        A,cache = linear_activation_forward(A_prev, parameters['W' + str(l)],\n",
    "                                            parameters['b' + str(l)] , \"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "    # AL means output layer\n",
    "    AL,cache = linear_activation_forward(A, parameters['W' + str(L)],\n",
    "                                            parameters['b' + str(L)] , \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    '''\n",
    "        now we have all layers actavation variable for each layer in caches\n",
    "        variable: A_prev,W,b,Z for each layer  \n",
    "        and last layer activation AL\n",
    "    '''\n",
    "    return AL, caches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "(3, 2)\n",
      "(1, 2)\n",
      "AL = [[0.38830879 0.58540103]]\n",
      "Length of caches list = 2\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(4,2)\n",
    "W1 = np.random.randn(3,4)\n",
    "b1 = np.random.randn(3,1)\n",
    "W2 = np.random.randn(1,3)\n",
    "b2 = np.random.randn(1,1)\n",
    "parameters = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2}\n",
    "\n",
    "AL, caches = Linear_forward_model_shallow_or_deep_NN(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_cost_function(AL, Y):\n",
    "    '''\n",
    "        At the end of forward process:\n",
    "        we calculate the cost function for last layer then we process to backward \n",
    "    '''\n",
    "    \n",
    "    m = Y.shape[1] # get number of training examples\n",
    "    cost = (-1/m) * np.sum((np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.2797765635793422\n"
     ]
    }
   ],
   "source": [
    "Y = np.asarray([[1, 1, 0]])\n",
    "AL = np.array([[.8,.9,0.4]])\n",
    "\n",
    "print(\"cost = \" + str(forward_cost_function(AL, Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 0.77669034 -1.88132907 -0.65140192 -0.55878017]\n",
      " [ 1.33952232  0.40764844 -0.75963566  0.10429644]\n",
      " [-2.82534175 -1.52885908  1.30192054 -1.12570307]\n",
      " [-0.64062238  0.11938664 -0.25193898 -1.92267442]\n",
      " [-0.28589563  0.29024029  0.01835481 -0.46303988]]\n",
      "dW = [[-0.20686586 -0.42776967  0.10987313  0.00932069 -0.24162395]\n",
      " [-0.20670748  0.28502228  0.51533545  0.11976457 -0.49756057]\n",
      " [ 0.74681502  0.25294116  0.08213842  0.1887332  -0.57425581]]\n",
      "db = [[-0.2410353 ]\n",
      " [-0.46228445]\n",
      " [-0.58997865]]\n"
     ]
    }
   ],
   "source": [
    "dZ = np.random.randn(3,4)\n",
    "A = np.random.randn(5,4)\n",
    "W = np.random.randn(3,5)\n",
    "b = np.random.randn(3,1)\n",
    "linear_cache = (A, W, b)\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation_fun_name):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation_fun_name == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    elif activation_fun_name == \"relu\":\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[0.20534043 0.70022212]\n",
      " [0.04860888 0.16575893]\n",
      " [0.02522553 0.08602045]]\n",
      "dW = [[-0.15884885 -0.24187055  0.01191697]]\n",
      "db = [[-0.17993903]]\n",
      "\n",
      "=====================================\n",
      "relu:\n",
      "dA_prev = [[1.58386634 2.80174356]\n",
      " [0.37493815 0.66323812]\n",
      " [0.19457382 0.34418684]]\n",
      "dW = [[-0.62537927 -0.96360787  0.1650769 ]]\n",
      "db = [[-0.87143885]]\n"
     ]
    }
   ],
   "source": [
    "dAL = np.random.randn(1,2)\n",
    "A = np.random.randn(3,2)\n",
    "W = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "Z = np.random.randn(1,2)\n",
    "linear_cache = (A, W, b)\n",
    "activation_cache = Z\n",
    "linear_activation_cache = (linear_cache, activation_cache)\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache,\"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "print(\"=====================================\")\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_backward_model_shallow_or_deep_NN(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    L = len(caches)\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    m = AL.shape[1]\n",
    "    grades = {}\n",
    "    dAL = -(np.divide(Y,AL)) - (np.divide((1-Y), (1-AL)))\n",
    "    dA_prev, dW, db = linear_activation_backward(dAL, caches[L-1], \"sigmoid\")\n",
    "    grades['dA' + str(L-1)], grades['dW' + str(L)], grades['db' + str(L)]  = dA_prev, dW, db\n",
    "    for l in reversed(range(L-1)):\n",
    "        cuurent_cache = caches[l]\n",
    "        dA_prev, dW, db = linear_activation_backward(grades['dA' + str(l+1)],cuurent_cache, \"sigmoid\")\n",
    "        grades['dA' + str(l)], grades['dW' + str(l+1)], grades['db' + str(l+1)]  = dA_prev, dW, db\n",
    "    \n",
    "    return grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[ 1.58545954e-03 -1.40949465e-02 -1.29782508e-02 -8.80901444e-03]\n",
      " [ 2.49625221e-03 -1.62157979e-02 -1.49536643e-02 -1.10810514e-02]\n",
      " [-6.34542015e-05  7.11808671e-04  6.54856160e-04  4.21471372e-04]]\n",
      "db1 = [[ 2.14132099e-03]\n",
      " [ 3.11373435e-03]\n",
      " [-9.20699198e-05]]\n",
      "dA1 = [[ 0.08273794 -0.05459794]\n",
      " [ 0.08234898 -0.05434127]\n",
      " [-0.00497779  0.0032848 ]]\n"
     ]
    }
   ],
   "source": [
    "AL = np.random.randn(1, 2)\n",
    "Y = np.array([[1, 0]])\n",
    "\n",
    "A1 = np.random.randn(4,2)\n",
    "W1 = np.random.randn(3,4)\n",
    "b1 = np.random.randn(3,1)\n",
    "Z1 = np.random.randn(3,2)\n",
    "linear_cache_activation_1 = ((A1, W1, b1), Z1)\n",
    "\n",
    "A2 = np.random.randn(3,2)\n",
    "W2 = np.random.randn(1,3)\n",
    "b2 = np.random.randn(1,1)\n",
    "Z2 = np.random.randn(1,2)\n",
    "linear_cache_activation_2 = ((A2, W2, b2), Z2)\n",
    "\n",
    "caches = (linear_cache_activation_1, linear_cache_activation_2)\n",
    "\n",
    "grads = Linear_backward_model_shallow_or_deep_NN(AL, Y, caches)\n",
    "print_grads(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (â‰ˆ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\"+str(l+1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\"+str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    ### END CODE HERE ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-1.17487374 -1.2395125   2.49633054  2.10614156]\n",
      " [-1.36395506  0.48569364  0.63571123  0.8140373 ]\n",
      " [ 0.14477725 -0.87915014 -0.05103393 -0.57958738]]\n",
      "b1 = [[ 0.04624131]\n",
      " [-0.43932861]\n",
      " [-0.13667899]]\n",
      "W2 = [[ 1.05902022  1.28509419 -1.02645544]]\n",
      "b2 = [[-0.55816685]]\n"
     ]
    }
   ],
   "source": [
    "W1 = np.random.randn(3,4)\n",
    "b1 = np.random.randn(3,1)\n",
    "W2 = np.random.randn(1,3)\n",
    "b2 = np.random.randn(1,1)\n",
    "parameters = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2}\n",
    "np.random.seed(3)\n",
    "dW1 = np.random.randn(3,4)\n",
    "db1 = np.random.randn(3,1)\n",
    "dW2 = np.random.randn(1,3)\n",
    "db2 = np.random.randn(1,1)\n",
    "grads = {\"dW1\": dW1,\n",
    "         \"db1\": db1,\n",
    "         \"dW2\": dW2,\n",
    "         \"db2\": db2}\n",
    "    \n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
